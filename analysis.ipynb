{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# T-ECD Dataset Analysis\n",
                "\n",
                "This notebook loads, joins, and analyzes the T-ECD dataset from Hugging Face.\n",
                "It handles partitioned datasets (e.g., daily event files) by loading a configurable number of partitions and concatenating them.\n",
                "It also includes the **Payments** dataset from the 'full' partition, as it is missing from the 'small' partition."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary libraries\n",
                "!pip install huggingface_hub pandas pyarrow ipywidgets matplotlib seaborn"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Authentication\n",
                "You need a Hugging Face token to access the dataset. \n",
                "1. Go to https://huggingface.co/settings/tokens\n",
                "2. Create a new token (Read access is sufficient)\n",
                "3. Paste it below when prompted."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration & Helper Functions\n",
                "\n",
                "We define global constants to control the data loading process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import hf_hub_download, list_repo_files\n",
                "import pandas as pd\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from collections import defaultdict\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "REPO_ID = \"t-tech/T-ECD\"\n",
                "REPO_TYPE = \"dataset\"\n",
                "CACHE_DIR = \"dataset_cache\"  # Local folder to store downloaded files\n",
                "\n",
                "# Dataset Paths\n",
                "DATASET_PATH_SMALL = \"dataset/small\"\n",
                "DATASET_PATH_FULL = \"dataset/full\" # Used for missing payments data\n",
                "\n",
                "# GLOBAL CONSTANT: How many partitions to load for split datasets (e.g., events)\n",
                "# Set this to a higher number (e.g., 50 or 100) to analyze more data.\n",
                "# Set to None to load ALL available partitions (Warning: May run out of RAM in Colab)\n",
                "NUM_PARTITIONS_TO_LOAD = 5 \n",
                "\n",
                "def load_remote_parquet(filename):\n",
                "    \"\"\"\n",
                "    Downloads a single parquet file from the HF repo to a local cache folder \n",
                "    and loads it into a Pandas DataFrame.\n",
                "    \"\"\"\n",
                "    print(f\"Downloading {filename} to {CACHE_DIR}...\")\n",
                "    try:\n",
                "        local_path = hf_hub_download(\n",
                "            repo_id=REPO_ID,\n",
                "            filename=filename,\n",
                "            repo_type=REPO_TYPE,\n",
                "            local_dir=CACHE_DIR,\n",
                "            local_dir_use_symlinks=False\n",
                "        )\n",
                "        # print(f\"File cached at: {local_path}\")\n",
                "        df = pd.read_parquet(local_path)\n",
                "        return df\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading {filename}: {e}\")\n",
                "        return None\n",
                "\n",
                "def load_dataframe_from_partitions(file_list, limit=NUM_PARTITIONS_TO_LOAD):\n",
                "    \"\"\"\n",
                "    Loads multiple parquet files from a list and concatenates them into a single DataFrame.\n",
                "    \"\"\"\n",
                "    if not file_list:\n",
                "        print(\"No files provided to load.\")\n",
                "        return None\n",
                "    \n",
                "    # Sort files to ensure order (e.g., by date)\n",
                "    sorted_files = sorted(file_list)\n",
                "    \n",
                "    # Apply limit\n",
                "    if limit is not None:\n",
                "        files_to_load = sorted_files[:limit]\n",
                "        print(f\"Loading {len(files_to_load)} partitions (out of {len(sorted_files)} available)...\")\n",
                "    else:\n",
                "        files_to_load = sorted_files\n",
                "        print(f\"Loading ALL {len(files_to_load)} partitions...\")\n",
                "\n",
                "    dfs = []\n",
                "    for f in files_to_load:\n",
                "        df = load_remote_parquet(f)\n",
                "        if df is not None:\n",
                "            dfs.append(df)\n",
                "    \n",
                "    if not dfs:\n",
                "        return None\n",
                "    \n",
                "    print(\"Concatenating partitions...\")\n",
                "    full_df = pd.concat(dfs, ignore_index=True)\n",
                "    return full_df\n",
                "\n",
                "def analyze_dataframe(df, name=\"DataFrame\"):\n",
                "    \"\"\"\n",
                "    Performs standard data analysis steps from the lecture notes,\n",
                "    including statistical summary, missing values, duplicates, and visualizations.\n",
                "    \"\"\"\n",
                "    if df is None:\n",
                "        print(f\"{name} is None, skipping analysis.\")\n",
                "        return\n",
                "\n",
                "    print(f\"\\n\" + \"=\"*20 + f\" ANALYZING: {name} \" + \"=\"*20)\n",
                "    print(f\"Shape: {df.shape}\")\n",
                "    \n",
                "    print(\"\\n1. Head (First 5 rows):\")\n",
                "    display(df.head())\n",
                "    \n",
                "    print(\"\\n2. Info (Data Types & Non-Null Counts):\")\n",
                "    df.info()\n",
                "    \n",
                "    print(\"\\n3. Describe (Statistical Summary for Numeric Columns):\")\n",
                "    display(df.describe())\n",
                "    \n",
                "    print(\"\\n4. Missing Values (NaN Count):\")\n",
                "    print(df.isnull().sum())\n",
                "    \n",
                "    print(\"\\n5. Duplicates Count:\")\n",
                "    try:\n",
                "        print(df.duplicated().sum())\n",
                "    except TypeError:\n",
                "        print(\"  Warning: Unable to check for duplicates due to unhashable types (e.g., embeddings).\")\n",
                "        if 'embedding' in df.columns:\n",
                "            print(\"  Retrying without 'embedding' column:\")\n",
                "            print(f\"  {df.drop(columns=['embedding']).duplicated().sum()}\")\n",
                "    \n",
                "    print(\"\\n6. Column Value Counts (Top 5 unique values for object columns):\")\n",
                "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
                "    for col in cat_cols:\n",
                "        if col == \"embedding\":\n",
                "            print(f\"  Skipping value counts for '{col}' column (performance optimization).\")\n",
                "            continue\n",
                "        print(f\"\\nColumn: {col}\")\n",
                "        print(df[col].value_counts().head())\n",
                "\n",
                "    print(\"\\n7. Visualizations:\")\n",
                "    \n",
                "    # Sample for visualization to avoid timeouts on large datasets\n",
                "    SAMPLE_SIZE = 10000\n",
                "    if len(df) > SAMPLE_SIZE:\n",
                "        print(f\"  (Using a random sample of {SAMPLE_SIZE} rows for plotting to improve performance)\")\n",
                "        plot_df = df.sample(SAMPLE_SIZE)\n",
                "    else:\n",
                "        plot_df = df\n",
                "\n",
                "    # Numeric Distributions & Timedelta\n",
                "    num_cols = df.select_dtypes(include=['number', 'timedelta']).columns\n",
                "    if len(num_cols) > 0:\n",
                "        print(f\"  - Plotting distributions for numeric/timedelta columns: {list(num_cols)}\")\n",
                "        for col in num_cols:\n",
                "            # Skip specific columns that cause issues or are not useful distributions\n",
                "            if \"id\" in col.lower() and df[col].nunique() > 1000:\n",
                "                 print(f\"    Skipping distribution plot for {col} (likely an ID with high cardinality).\")\n",
                "                 continue\n",
                "            \n",
                "            plt.figure(figsize=(8, 4))\n",
                "            series_to_plot = plot_df[col].dropna()\n",
                "            \n",
                "            # Handle timedelta: Convert to total seconds\n",
                "            if pd.api.types.is_timedelta64_dtype(series_to_plot):\n",
                "                print(f\"    Converting {col} to total seconds for plotting.\")\n",
                "                series_to_plot = series_to_plot.dt.total_seconds()\n",
                "            \n",
                "            sns.histplot(series_to_plot, kde=True, bins=30)\n",
                "            plt.title(f\"Distribution of {col}\")\n",
                "            plt.xlabel(col)\n",
                "            plt.ylabel(\"Frequency\")\n",
                "            plt.show()\n",
                "            \n",
                "    # Categorical Counts (Top 10)\n",
                "    if len(cat_cols) > 0:\n",
                "        print(f\"  - Plotting counts for categorical columns: {list(cat_cols)}\")\n",
                "        for col in cat_cols:\n",
                "            if col == \"embedding\":\n",
                "                continue # Skip embedding plots\n",
                "            if df[col].nunique() > 50: # Skip if too many unique values\n",
                "                print(f\"    Skipping plot for {col} (too many unique values: {df[col].nunique()})\")\n",
                "                continue\n",
                "            plt.figure(figsize=(10, 5))\n",
                "            # Use full dataframe for determining the top 10 order to be accurate\n",
                "            top_10_order = df[col].value_counts().iloc[:10].index\n",
                "            # Plot using the sample (or full if small)\n",
                "            sns.countplot(y=col, data=plot_df, order=top_10_order)\n",
                "            plt.title(f\"Top 10 Counts for {col}\")\n",
                "            plt.xlabel(\"Count\")\n",
                "            plt.ylabel(col)\n",
                "            plt.show()\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Explore Repository & Index Files\n",
                "We list all files once and categorize them to avoid repeated API calls."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_files = list_repo_files(repo_id=REPO_ID, repo_type=REPO_TYPE)\n",
                "\n",
                "# Categorize files by domain and type for easy access\n",
                "dataset_files = defaultdict(list)\n",
                "\n",
                "for f in all_files:\n",
                "    if f.endswith(\".pq\"):\n",
                "        # Example f: dataset/small/retail/events/01082.pq\n",
                "        # Key: dataset/small/retail/events\n",
                "        dirname = os.path.dirname(f).replace(\"\\\\\", \"/\") # Normalize path separators\n",
                "        dataset_files[dirname].append(f)\n",
                "\n",
                "print(\"File Index Created. Available Directories:\")\n",
                "for d in sorted(dataset_files.keys()):\n",
                "    count = len(dataset_files[d])\n",
                "    print(f\" - {d} ({count} files)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Static Data Analysis (Users & Brands)\n",
                "These are single files found in the root of the dataset partition."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- USERS ---\n",
                "users_path = f\"{DATASET_PATH_SMALL}/users.pq\"\n",
                "df_users = load_remote_parquet(users_path)\n",
                "analyze_dataframe(df_users, \"Users Data\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- BRANDS ---\n",
                "brands_path = f\"{DATASET_PATH_SMALL}/brands.pq\"\n",
                "df_brands = load_remote_parquet(brands_path)\n",
                "\n",
                "# Handle broken brands file (empty embeddings)\n",
                "if df_brands is None:\n",
                "    print(\"Attempting to load Brands without 'embedding' column due to schema error...\")\n",
                "    local_path = f\"{CACHE_DIR}/{brands_path}\"\n",
                "    if os.path.exists(local_path):\n",
                "        try:\n",
                "            df_brands = pd.read_parquet(local_path, columns=['brand_id'])\n",
                "            print(\"Successfully loaded Brands data (excluding embeddings).\")\n",
                "        except Exception as e:\n",
                "            print(f\"Fallback load failed: {e}\")\n",
                "\n",
                "analyze_dataframe(df_brands, \"Brands Data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Retail Domain Analysis\n",
                "Contains `items` (static) and `events` (partitioned)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- RETAIL ITEMS ---\n",
                "retail_items_path = f\"{DATASET_PATH_SMALL}/retail/items.pq\"\n",
                "df_retail_items = load_remote_parquet(retail_items_path)\n",
                "analyze_dataframe(df_retail_items, \"Retail Items\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- RETAIL EVENTS ---\n",
                "retail_events_dir = f\"{DATASET_PATH_SMALL}/retail/events\"\n",
                "retail_event_files = dataset_files.get(retail_events_dir, [])\n",
                "\n",
                "df_retail_events = load_dataframe_from_partitions(retail_event_files)\n",
                "analyze_dataframe(df_retail_events, \"Retail Events (Joined)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Marketplace Domain Analysis\n",
                "Contains `items` (static) and `events` (partitioned)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- MARKETPLACE ITEMS ---\n",
                "mp_items_path = f\"{DATASET_PATH_SMALL}/marketplace/items.pq\"\n",
                "df_mp_items = load_remote_parquet(mp_items_path)\n",
                "analyze_dataframe(df_mp_items, \"Marketplace Items\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- MARKETPLACE EVENTS ---\n",
                "mp_events_dir = f\"{DATASET_PATH_SMALL}/marketplace/events\"\n",
                "mp_event_files = dataset_files.get(mp_events_dir, [])\n",
                "\n",
                "df_mp_events = load_dataframe_from_partitions(mp_event_files)\n",
                "analyze_dataframe(df_mp_events, \"Marketplace Events (Joined)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Offers Domain Analysis\n",
                "Contains `items` (static) and `events` (partitioned)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- OFFERS ITEMS ---\n",
                "offers_items_path = f\"{DATASET_PATH_SMALL}/offers/items.pq\"\n",
                "df_offers_items = load_remote_parquet(offers_items_path)\n",
                "analyze_dataframe(df_offers_items, \"Offers Items\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- OFFERS EVENTS ---\n",
                "offers_events_dir = f\"{DATASET_PATH_SMALL}/offers/events\"\n",
                "offers_event_files = dataset_files.get(offers_events_dir, [])\n",
                "\n",
                "df_offers_events = load_dataframe_from_partitions(offers_event_files)\n",
                "analyze_dataframe(df_offers_events, \"Offers Events (Joined)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Reviews Domain Analysis\n",
                "Reviews are partitioned by day directly in the folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- REVIEWS ---\n",
                "reviews_dir = f\"{DATASET_PATH_SMALL}/reviews\"\n",
                "reviews_files = dataset_files.get(reviews_dir, [])\n",
                "\n",
                "df_reviews = load_dataframe_from_partitions(reviews_files)\n",
                "analyze_dataframe(df_reviews, \"Reviews (Joined)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Payments Domain Analysis (From 'Full' Dataset)\n",
                "**Note:** The `payments` data is missing from the `dataset/small` partition. \n",
                "We will load a sample from `dataset/full` to ensure we cover this domain in our analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- PAYMENTS EVENTS ---\n",
                "pay_events_dir = f\"{DATASET_PATH_FULL}/payments/events\"\n",
                "pay_event_files = dataset_files.get(pay_events_dir, [])\n",
                "\n",
                "print(f\"Found {len(pay_event_files)} payment event files in FULL dataset. Loading sample...\")\n",
                "df_pay_events = load_dataframe_from_partitions(pay_event_files)\n",
                "analyze_dataframe(df_pay_events, \"Payments Events (Sample from Full)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- PAYMENTS RECEIPTS ---\n",
                "pay_receipts_dir = f\"{DATASET_PATH_FULL}/payments/receipts\"\n",
                "pay_receipts_files = dataset_files.get(pay_receipts_dir, [])\n",
                "\n",
                "print(f\"Found {len(pay_receipts_files)} payment receipt files in FULL dataset. Loading sample...\")\n",
                "df_pay_receipts = load_dataframe_from_partitions(pay_receipts_files)\n",
                "analyze_dataframe(df_pay_receipts, \"Payments Receipts (Sample from Full)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
