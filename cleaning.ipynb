{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Pipeline: T-ECD Dataset\n",
    "\n",
    "## Objective\n",
    "This notebook implements a reproducible data cleaning pipeline based on the findings from the Exploratory Data Analysis (`analysis.ipynb`).\n",
    "\n",
    "## Identified Issues & Cleaning Actions\n",
    "1.  **Schema Errors**: Brands, Marketplace, Reviews files fail to load due to `embedding` corruption. **Action**: Attempt load; fallback to exclude `embedding` if failed.\n",
    "2.  **Missing Values**:\n",
    "    *   `Users`: `socdem_cluster`, `region`. **Action**: Impute with `-1`.\n",
    "    *   `Retail Items`: `category`, `subcategory`. **Action**: Impute with \"Unknown\". `price`. **Action**: Drop rows.\n",
    "    *   `Payments`: `brand_id`. **Action**: Impute with `-1`.\n",
    "3.  **Duplicates**: `Brands`. **Action**: Deduplicate.\n",
    "4.  **Data Types**: `price` (negative values). **Action**: Retain.\n",
    "5.  \n",
    "\n",
    "### Data Semantics: Embeddings\n",
    "The `embedding` columns contain **300-dimensional vector representations** of items and brands. \n",
    "*   **Purpose**: These are critical for downstream AI tasks such as **recommendation systems** (calculating item-item similarity) and **customer segmentation** (clustering).\n",
    "*   **Status**: \n",
    "    *   **Retail Items**: Embeddings are valid and **must be preserved**.\n",
    "    *   **Brands / Marketplace / Reviews**: The source Parquet files contain schema inconsistencies (e.g., empty lists mixed with 300-float lists), causing standard `pyarrow` loaders to fail.\n",
    "*   **Strategy**: We attempt to load the full datasets. If (and only if) the file is corrupted and loading fails, we fallback to excluding the `embedding` column for that specific file to salvage the remaining data (IDs, metadata). This ensures we don't discard the entire dataset due to one corrupted feature.\n",
    "\n",
    "#### Why not fix the embeddings?\n",
    "Replacing corrupted embeddings with dummy values is not feasible using standard `pyarrow` loaders because schema resolution fails before the column can be read into memory. While low-level or non-standard recovery approaches exist, they are operationally complex and disproportionate to the value gained. Therefore, excluding the embedding column for affected files is the most robust and cost-effective strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "REPO_ID = \"t-tech/T-ECD\"\n",
    "REPO_TYPE = \"dataset\"\n",
    "CACHE_DIR = \"dataset_cache\"\n",
    "DATASET_PATH_SMALL = \"dataset/small\"\n",
    "DATASET_PATH_FULL = \"dataset/full\"\n",
    "\n",
    "# GLOBAL CONSTANT: How many partitions to load for split datasets (e.g., events)\n",
    "# Set this to a higher number (e.g., 50 or 100) to analyze more data.\n",
    "# Set to None to load ALL available partitions (Warning: May run out of RAM in Colab)\n",
    "DATASET_SMALL_NUM_PARTITIONS_TO_LOAD = 5 \n",
    "DATASET_FULL_NUM_PARTITIONS_TO_LOAD = 1\n",
    "\n",
    "# Ensure cache directory exists\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Index Created. Available Directories:\n",
      " - dataset/full (2 files)\n",
      " - dataset/full/marketplace (1 files)\n",
      " - dataset/full/marketplace/events (793 files)\n",
      " - dataset/full/offers (1 files)\n",
      " - dataset/full/offers/events (944 files)\n",
      " - dataset/full/payments/events (1309 files)\n",
      " - dataset/full/payments/receipts (1017 files)\n",
      " - dataset/full/retail (1 files)\n",
      " - dataset/full/retail/events (579 files)\n",
      " - dataset/full/reviews (1309 files)\n",
      " - dataset/small (2 files)\n",
      " - dataset/small/marketplace (1 files)\n",
      " - dataset/small/marketplace/events (227 files)\n",
      " - dataset/small/offers (1 files)\n",
      " - dataset/small/offers/events (227 files)\n",
      " - dataset/small/retail (1 files)\n",
      " - dataset/small/retail/events (227 files)\n",
      " - dataset/small/reviews (227 files)\n"
     ]
    }
   ],
   "source": [
    "all_files = list_repo_files(repo_id=REPO_ID, repo_type=REPO_TYPE)\n",
    "\n",
    "# Categorize files by domain and type for easy access\n",
    "dataset_files = defaultdict(list)\n",
    "\n",
    "for f in all_files:\n",
    "    if f.endswith(\".pq\"):\n",
    "        # Example f: dataset/small/retail/events/01082.pq\n",
    "        # Key: dataset/small/retail/events\n",
    "        dirname = os.path.dirname(f).replace(\"\\\\\", \"/\") # Normalize path separators\n",
    "        dataset_files[dirname].append(f)\n",
    "\n",
    "print(\"File Index Created. Available Directories:\")\n",
    "for d in sorted(dataset_files.keys()):\n",
    "    count = len(dataset_files[d])\n",
    "    print(f\" - {d} ({count} files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_remote_parquet_safe(filename, columns_to_exclude=None):\n",
    "    \"\"\"\n",
    "    Loads a parquet file from Hugging Face, handling potential schema errors.\n",
    "    If columns_to_exclude is provided, those columns are excluded BEFORE loading.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {filename}...\")\n",
    "    try:\n",
    "        local_path = hf_hub_download(\n",
    "            repo_id=REPO_ID,\n",
    "            filename=filename,\n",
    "            repo_type=REPO_TYPE,\n",
    "            local_dir=CACHE_DIR,\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        \n",
    "        if columns_to_exclude:\n",
    "            import pyarrow.parquet as pq\n",
    "            # Read schema using read_schema (more reliable than ParquetFile)\n",
    "            schema = pq.read_schema(local_path)\n",
    "            # Filter out internal pyarrow columns (those starting with __)\n",
    "            all_cols = [name for name in schema.names if not name.startswith('__')]\n",
    "            use_cols = [c for c in all_cols if c not in columns_to_exclude]\n",
    "            print(f\"  Excluding columns: {columns_to_exclude}\")\n",
    "            print(f\"  Reading columns: {use_cols}\")\n",
    "            df = pd.read_parquet(local_path, columns=use_cols)\n",
    "            return df\n",
    "        \n",
    "        # Otherwise, try loading all columns\n",
    "        try:\n",
    "            df = pd.read_parquet(local_path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"  Standard load failed: {e}\")\n",
    "            import pyarrow.parquet as pq\n",
    "            schema = pq.read_schema(local_path)\n",
    "            all_cols = [name for name in schema.names if not name.startswith('__')]\n",
    "            if 'embedding' in all_cols:\n",
    "                print(f\"  Retrying without 'embedding' column...\")\n",
    "                use_cols = [c for c in all_cols if c != 'embedding']\n",
    "                df = pd.read_parquet(local_path, columns=use_cols)\n",
    "                print(\"  Success (with exclusions).\")\n",
    "                return df\n",
    "            else:\n",
    "                raise e\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading/loading {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_cleaning(df_before, df_after, name):\n",
    "    print(f\"\\n--- Validation: {name} ---\")\n",
    "    print(f\"Rows: {len(df_before)} -> {len(df_after)} (Dropped: {len(df_before) - len(df_after)})\")\n",
    "    print(f\"Missing Values Before: {df_before.isnull().sum().sum()}\")\n",
    "    print(f\"Missing Values After:  {df_after.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Exclude unhashable columns (like embedding) when checking duplicates\n",
    "    hashable_cols_before = [c for c in df_before.columns if df_before[c].dtype != 'object' or not df_before[c].apply(lambda x: isinstance(x, (list, np.ndarray))).any()]\n",
    "    hashable_cols_after = [c for c in df_after.columns if df_after[c].dtype != 'object' or not df_after[c].apply(lambda x: isinstance(x, (list, np.ndarray))).any()]\n",
    "    \n",
    "    try:\n",
    "        print(f\"Duplicate Rows Before: {df_before[hashable_cols_before].duplicated().sum()}\")\n",
    "        print(f\"Duplicate Rows After:  {df_after[hashable_cols_after].duplicated().sum()}\")\n",
    "    except:\n",
    "        print(\"Duplicate Rows: (skipped - unhashable columns present)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe_from_partitions_safe(file_list, limit=DATASET_SMALL_NUM_PARTITIONS_TO_LOAD, columns_to_exclude=None):\n",
    "    \"\"\"\n",
    "    Loads multiple parquet partition files and concatenates them.\n",
    "    Supports columns_to_exclude to skip corrupted columns like 'embedding'.\n",
    "    \"\"\"\n",
    "    if not file_list:\n",
    "        print(\"No files to load.\")\n",
    "        return None\n",
    "    \n",
    "    files_to_load = file_list[:limit] if limit else file_list\n",
    "    print(f\"Loading {len(files_to_load)} partitions (out of {len(file_list)} available)...\")\n",
    "    \n",
    "    dfs = []\n",
    "    for f in files_to_load:\n",
    "        df = load_remote_parquet_safe(f, columns_to_exclude=columns_to_exclude)\n",
    "        if df is not None:\n",
    "            dfs.append(df)\n",
    "    \n",
    "    if not dfs:\n",
    "        print(\"No dataframes loaded successfully.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Concatenating partitions...\")\n",
    "    full_df = pd.concat(dfs, ignore_index=True)\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Users Data Cleaning\n",
    "**Issue**: `socdem_cluster` and `region` contain missing values.\n",
    "**Strategy**: Impute with `-1` to represent \"Unknown\". Dropping these users might lose valuable event data linked to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset/small/users.pq...\n",
      "Before Cleaning:\n",
      "user_id               0\n",
      "socdem_cluster     5153\n",
      "region            58917\n",
      "dtype: int64\n",
      "Evidence - Missing socdem_cluster: 5153\n",
      "Evidence - Missing region: 58917\n",
      "\n",
      "--- Validation: Users ---\n",
      "Rows: 3500000 -> 3500000 (Dropped: 0)\n",
      "Missing Values Before: 64070\n",
      "Missing Values After:  0\n",
      "Duplicate Rows Before: 0\n",
      "Duplicate Rows After:  0\n"
     ]
    }
   ],
   "source": [
    "users_path = f\"{DATASET_PATH_SMALL}/users.pq\"\n",
    "df_users = load_remote_parquet_safe(users_path)\n",
    "\n",
    "if df_users is not None:\n",
    "    print(\"Before Cleaning:\")\n",
    "    print(df_users.isnull().sum())\n",
    "    \n",
    "    # Evidence\n",
    "    print(f\"Evidence - Missing socdem_cluster: {df_users['socdem_cluster'].isnull().sum()}\")\n",
    "    print(f\"Evidence - Missing region: {df_users['region'].isnull().sum()}\")\n",
    "    \n",
    "    # Cleaning\n",
    "    df_users_clean = df_users.copy()\n",
    "    df_users_clean['socdem_cluster'] = df_users_clean['socdem_cluster'].fillna(-1)\n",
    "    df_users_clean['region'] = df_users_clean['region'].fillna(-1)\n",
    "    \n",
    "    validate_cleaning(df_users, df_users_clean, \"Users\")\n",
    "    \n",
    "    # Verify\n",
    "    assert df_users_clean.isnull().sum().sum() == 0, \"Users table still has missing values!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Brands Data Cleaning\n",
    "**Issue 1**: Schema error when loading `embedding` column.\n",
    "**Strategy**: Load without `embedding`.\n",
    "**Issue 2**: Duplicate `brand_id` entries.\n",
    "**Strategy**: Drop duplicates to ensure `brand_id` is a unique primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset/small/brands.pq...\n",
      "  Excluding columns: ['embedding']\n",
      "  Reading columns: ['brand_id']\n",
      "Initial Rows: 24513\n",
      "Duplicates found: 46\n",
      "Evidence - Duplicate brand_ids: 46\n",
      "\n",
      "--- Validation: Brands ---\n",
      "Rows: 24513 -> 24467 (Dropped: 46)\n",
      "Missing Values Before: 0\n",
      "Missing Values After:  0\n",
      "Duplicate Rows Before: 46\n",
      "Duplicate Rows After:  0\n"
     ]
    }
   ],
   "source": [
    "brands_path = f\"{DATASET_PATH_SMALL}/brands.pq\"\n",
    "# Explicitly handle the known schema error by excluding embedding if needed\n",
    "df_brands = load_remote_parquet_safe(brands_path, columns_to_exclude=['embedding'])\n",
    "\n",
    "if df_brands is not None:\n",
    "    print(f\"Initial Rows: {len(df_brands)}\")\n",
    "    \n",
    "    # Check Duplicates\n",
    "    dupes = df_brands.duplicated(subset=['brand_id']).sum()\n",
    "    print(f\"Duplicates found: {dupes}\")\n",
    "    \n",
    "    # Evidence\n",
    "    print(f\"Evidence - Duplicate brand_ids: {dupes}\")\n",
    "    \n",
    "    # Cleaning\n",
    "    df_brands_clean = df_brands.drop_duplicates(subset=['brand_id']).copy()\n",
    "    \n",
    "    validate_cleaning(df_brands, df_brands_clean, \"Brands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Retail Items Cleaning\n",
    "**Issue**: Missing `category`, `subcategory`, and `price`.\n",
    "**Strategy**:\n",
    "*   `category`/`subcategory`: Impute with \"Unknown\".\n",
    "*   `price`: Drop rows. Price is a critical feature for most analyses; imputing it (e.g., with mean) could introduce significant bias given the wide range of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset/small/retail/items.pq...\n",
      "Before Cleaning:\n",
      "item_id            0\n",
      "brand_id           0\n",
      "category        9585\n",
      "subcategory     9585\n",
      "price          26489\n",
      "embedding          0\n",
      "dtype: int64\n",
      "Evidence - Missing category: 9585\n",
      "Evidence - Missing subcategory: 9585\n",
      "Evidence - Missing price: 26489\n",
      "\n",
      "--- Validation: Retail Items ---\n",
      "Rows: 250171 -> 223682 (Dropped: 26489)\n",
      "Missing Values Before: 45659\n",
      "Missing Values After:  0\n",
      "Duplicate Rows Before: 0\n",
      "Duplicate Rows After:  0\n"
     ]
    }
   ],
   "source": [
    "retail_items_path = f\"{DATASET_PATH_SMALL}/retail/items.pq\"\n",
    "df_retail_items = load_remote_parquet_safe(retail_items_path)\n",
    "\n",
    "if df_retail_items is not None:\n",
    "    print(\"Before Cleaning:\")\n",
    "    print(df_retail_items.isnull().sum())\n",
    "    \n",
    "    # Evidence\n",
    "    print(f\"Evidence - Missing category: {df_retail_items['category'].isnull().sum()}\")\n",
    "    print(f\"Evidence - Missing subcategory: {df_retail_items['subcategory'].isnull().sum()}\")\n",
    "    print(f\"Evidence - Missing price: {df_retail_items['price'].isnull().sum()}\")\n",
    "    \n",
    "    df_retail_clean = df_retail_items.copy()\n",
    "    \n",
    "    # Impute Categorical\n",
    "    df_retail_clean['category'] = df_retail_clean['category'].fillna(\"Unknown\")\n",
    "    df_retail_clean['subcategory'] = df_retail_clean['subcategory'].fillna(\"Unknown\")\n",
    "    \n",
    "    # Drop Missing Price\n",
    "    df_retail_clean = df_retail_clean.dropna(subset=['price'])\n",
    "    \n",
    "    validate_cleaning(df_retail_items, df_retail_clean, \"Retail Items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Retail Events (Validation Only)\n",
    "**Issue**: Analysis shows 0 missing values.\n",
    "**Strategy**: No cleaning needed. Validate data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 5 partitions (out of 227 available)...\n",
      "Loading dataset/small/retail/events/01082.pq...\n",
      "Loading dataset/small/retail/events/01083.pq...\n",
      "Loading dataset/small/retail/events/01084.pq...\n",
      "Loading dataset/small/retail/events/01085.pq...\n",
      "Loading dataset/small/retail/events/01086.pq...\n",
      "Concatenating partitions...\n",
      "Validation Check:\n",
      "timestamp      0\n",
      "user_id        0\n",
      "item_id        0\n",
      "subdomain      0\n",
      "action_type    0\n",
      "os             0\n",
      "dtype: int64\n",
      "\n",
      "Total Missing Values: 0\n",
      "Shape: (1892095, 6)\n"
     ]
    }
   ],
   "source": [
    "# --- RETAIL EVENTS ---\n",
    "retail_events_dir = f\"{DATASET_PATH_SMALL}/retail/events\"\n",
    "retail_events_files = dataset_files.get(retail_events_dir, [])\n",
    "\n",
    "df_retail_events = load_dataframe_from_partitions_safe(retail_events_files, limit=5)\n",
    "\n",
    "if df_retail_events is not None:\n",
    "    print(\"Validation Check:\")\n",
    "    print(df_retail_events.isnull().sum())\n",
    "    print(f\"\\nTotal Missing Values: {df_retail_events.isnull().sum().sum()}\")\n",
    "    print(f\"Shape: {df_retail_events.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Marketplace Items Cleaning\n",
    "**Issue**: Schema error (embedding).\n",
    "**Strategy**: Load without `embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset/small/marketplace/items.pq...\n",
      "  Excluding columns: ['embedding']\n",
      "  Reading columns: ['item_id', 'brand_id', 'category', 'subcategory', 'price']\n",
      "Successfully loaded Marketplace Items. Shape: (2325409, 5)\n"
     ]
    }
   ],
   "source": [
    "mp_items_path = f\"{DATASET_PATH_SMALL}/marketplace/items.pq\"\n",
    "df_mp_items = load_remote_parquet_safe(mp_items_path, columns_to_exclude=['embedding'])\n",
    "\n",
    "if df_mp_items is not None:\n",
    "    print(f\"Successfully loaded Marketplace Items. Shape: {df_mp_items.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Marketplace Events Cleaning\n",
    "**Issue**: `subdomain` column has 1115 missing values.\n",
    "**Strategy**: Impute with `\"Unknown\"` to preserve all events for behavioral analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 5 partitions (out of 227 available)...\n",
      "Loading dataset/small/marketplace/events/01082.pq...\n",
      "Loading dataset/small/marketplace/events/01083.pq...\n",
      "Loading dataset/small/marketplace/events/01084.pq...\n",
      "Loading dataset/small/marketplace/events/01085.pq...\n",
      "Loading dataset/small/marketplace/events/01086.pq...\n",
      "Concatenating partitions...\n",
      "Before Cleaning:\n",
      "timestamp         0\n",
      "user_id           0\n",
      "item_id           0\n",
      "subdomain      1115\n",
      "action_type       0\n",
      "os                0\n",
      "dtype: int64\n",
      "\n",
      "Evidence - Missing subdomain: 1115\n",
      "\n",
      "--- Validation: Marketplace Events ---\n",
      "Rows: 2561575 -> 2561575 (Dropped: 0)\n",
      "Missing Values Before: 1115\n",
      "Missing Values After:  0\n",
      "Duplicate Rows Before: 0\n",
      "Duplicate Rows After:  0\n"
     ]
    }
   ],
   "source": [
    "mp_events_dir = f\"{DATASET_PATH_SMALL}/marketplace/events\"\n",
    "mp_events_files = dataset_files.get(mp_events_dir, [])\n",
    "\n",
    "df_mp_events = load_dataframe_from_partitions_safe(mp_events_files, limit=DATASET_SMALL_NUM_PARTITIONS_TO_LOAD)\n",
    "\n",
    "if df_mp_events is not None:\n",
    "    print(\"Before Cleaning:\")\n",
    "    print(df_mp_events.isnull().sum())\n",
    "    \n",
    "    # Evidence\n",
    "    print(f\"\\nEvidence - Missing subdomain: {df_mp_events['subdomain'].isnull().sum()}\")\n",
    "    \n",
    "    df_mp_events_clean = df_mp_events.copy()\n",
    "    df_mp_events_clean['subdomain'] = df_mp_events_clean['subdomain'].fillna(\"Unknown\")\n",
    "    \n",
    "    validate_cleaning(df_mp_events, df_mp_events_clean, \"Marketplace Events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reviews Cleaning\n",
    "**Issue**: Schema error (embedding) in partition files.\n",
    "**Strategy**: Load a sample partition without `embedding` to demonstrate the fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 5 partitions (out of 227 available)...\n",
      "Loading dataset/small/reviews/01082.pq...\n",
      "  Excluding columns: ['embedding']\n",
      "  Reading columns: ['timestamp', 'user_id', 'brand_id', 'rating']\n",
      "Loading dataset/small/reviews/01083.pq...\n",
      "  Excluding columns: ['embedding']\n",
      "  Reading columns: ['timestamp', 'user_id', 'brand_id', 'rating']\n",
      "Loading dataset/small/reviews/01084.pq...\n",
      "  Excluding columns: ['embedding']\n",
      "  Reading columns: ['timestamp', 'user_id', 'brand_id', 'rating']\n",
      "Loading dataset/small/reviews/01085.pq...\n",
      "  Excluding columns: ['embedding']\n",
      "  Reading columns: ['timestamp', 'user_id', 'brand_id', 'rating']\n",
      "Loading dataset/small/reviews/01086.pq...\n",
      "  Excluding columns: ['embedding']\n",
      "  Reading columns: ['timestamp', 'user_id', 'brand_id', 'rating']\n",
      "Concatenating partitions...\n",
      "Successfully loaded Reviews. Shape: (11545, 4)\n",
      "                  timestamp   user_id  brand_id  rating\n",
      "0 1082 days 00:01:05.711723  25741061    141226       5\n",
      "1 1082 days 00:01:12.026501  71011848     65693       3\n",
      "2 1082 days 00:02:15.540704  26101012     72285       5\n",
      "3 1082 days 00:02:25.126055  67977146    184380       3\n",
      "4 1082 days 00:02:36.993924   4055428    216938       5\n"
     ]
    }
   ],
   "source": [
    "# Get list of review partition files (similar to analysis.ipynb)\n",
    "reviews_dir = f\"{DATASET_PATH_SMALL}/reviews\"\n",
    "review_files = dataset_files.get(reviews_dir, [])  # Assumes you have the dataset_files dict from analysis.ipynb\n",
    "\n",
    "# Load with embedding excluded\n",
    "df_reviews = load_dataframe_from_partitions_safe(\n",
    "    review_files, \n",
    "    limit=DATASET_SMALL_NUM_PARTITIONS_TO_LOAD,  # Or whatever limit you want\n",
    "    columns_to_exclude=['embedding']\n",
    ")\n",
    "\n",
    "if df_reviews is not None:\n",
    "    print(f\"Successfully loaded Reviews. Shape: {df_reviews.shape}\")\n",
    "    print(df_reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Payments Data Cleaning\n",
    "**Issue**: High missingness in `brand_id` (~90%) and some missing `price`.\n",
    "**Strategy**:\n",
    "*   `brand_id`: Impute with `-1`. The high missing rate suggests many items are non-branded or the data is simply not available. Dropping would lose 90% of data.\n",
    "*   `price`: Drop rows with missing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 partitions (out of 1017 available)...\n",
      "Loading dataset/full/payments/receipts/00292.pq...\n",
      "Concatenating partitions...\n",
      "Before Cleaning:\n",
      "timestamp                   0\n",
      "user_id                     0\n",
      "brand_id               999928\n",
      "approximate_item_id         0\n",
      "count                       0\n",
      "price                   14892\n",
      "transaction_hash            0\n",
      "dtype: int64\n",
      "\n",
      "Evidence - Missing brand_id: 999928 (90.0%)\n",
      "Evidence - Missing price: 14892\n",
      "\n",
      "--- Validation: Payments Receipts ---\n",
      "Rows: 1110501 -> 1095609 (Dropped: 14892)\n",
      "Missing Values Before: 1014820\n",
      "Missing Values After:  0\n",
      "Duplicate Rows Before: 0\n",
      "Duplicate Rows After:  0\n"
     ]
    }
   ],
   "source": [
    "# Get list of receipt partition files\n",
    "receipts_dir = f\"{DATASET_PATH_FULL}/payments/receipts\"\n",
    "receipt_files = dataset_files.get(receipts_dir, [])\n",
    "\n",
    "# Load multiple partitions (no embedding issues here, so no columns_to_exclude needed)\n",
    "df_receipts = load_dataframe_from_partitions_safe(\n",
    "    receipt_files, \n",
    "    limit=DATASET_FULL_NUM_PARTITIONS_TO_LOAD,  # Adjust as needed (or set to None for all)\n",
    "    columns_to_exclude=None\n",
    ")\n",
    "\n",
    "if df_receipts is not None:\n",
    "    print(\"Before Cleaning:\")\n",
    "    print(df_receipts.isnull().sum())\n",
    "    \n",
    "    # Evidence\n",
    "    print(f\"\\nEvidence - Missing brand_id: {df_receipts['brand_id'].isnull().sum()} ({df_receipts['brand_id'].isnull().mean()*100:.1f}%)\")\n",
    "    print(f\"Evidence - Missing price: {df_receipts['price'].isnull().sum()}\")\n",
    "    \n",
    "    df_receipts_clean = df_receipts.copy()\n",
    "    \n",
    "    # Impute Brand ID\n",
    "    df_receipts_clean['brand_id'] = df_receipts_clean['brand_id'].fillna(-1)\n",
    "    \n",
    "    # Drop Missing Price\n",
    "    df_receipts_clean = df_receipts_clean.dropna(subset=['price'])\n",
    "    \n",
    "    validate_cleaning(df_receipts, df_receipts_clean, \"Payments Receipts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Payments Events (Validation Only)\n",
    "**Issue**: Not analyzed in detail in EDA. Need to verify data quality.\n",
    "**Strategy**: Load sample and check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PAYMENTS EVENTS ---\n",
    "pay_events_dir = f\"{DATASET_PATH_FULL}/payments/events\"\n",
    "pay_events_files = dataset_files.get(pay_events_dir, [])\n",
    "\n",
    "df_pay_events = load_dataframe_from_partitions_safe(pay_events_files, limit=1)\n",
    "\n",
    "if df_pay_events is not None:\n",
    "    print(\"Validation Check:\")\n",
    "    print(df_pay_events.isnull().sum())\n",
    "    print(f\"\\nTotal Missing Values: {df_pay_events.isnull().sum().sum()}\")\n",
    "    print(f\"Shape: {df_pay_events.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Offers Items Cleaning\n",
    "**Issue**: `brand_id` column has 542 missing values (2.4%).\n",
    "**Strategy**: Impute with `-1` to represent unbranded/unknown brand, preserving all offer items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OFFERS ITEMS ---\n",
    "offers_items_path = f\"{DATASET_PATH_SMALL}/offers/items.pq\"\n",
    "df_offers_items = load_remote_parquet_safe(offers_items_path)\n",
    "\n",
    "if df_offers_items is not None:\n",
    "    print(\"Before Cleaning:\")\n",
    "    print(df_offers_items.isnull().sum())\n",
    "    \n",
    "    # Evidence\n",
    "    print(f\"\\nEvidence - Missing brand_id: {df_offers_items['brand_id'].isnull().sum()} ({df_offers_items['brand_id'].isnull().mean()*100:.1f}%)\")\n",
    "    \n",
    "    df_offers_items_clean = df_offers_items.copy()\n",
    "    df_offers_items_clean['brand_id'] = df_offers_items_clean['brand_id'].fillna(-1)\n",
    "    \n",
    "    validate_cleaning(df_offers_items, df_offers_items_clean, \"Offers Items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Offers Events (Validation Only)\n",
    "**Issue**: Analysis shows 0 missing values.\n",
    "**Strategy**: No cleaning needed. Validate data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 5 partitions (out of 227 available)...\n",
      "Loading dataset/small/offers/events/01082.pq...\n",
      "Loading dataset/small/offers/events/01083.pq...\n",
      "Loading dataset/small/offers/events/01084.pq...\n",
      "Loading dataset/small/offers/events/01085.pq...\n",
      "Loading dataset/small/offers/events/01086.pq...\n",
      "Concatenating partitions...\n",
      "Validation Check:\n",
      "timestamp      0\n",
      "user_id        0\n",
      "item_id        0\n",
      "action_type    0\n",
      "dtype: int64\n",
      "\n",
      "Total Missing Values: 0\n",
      "Shape: (14814526, 4)\n"
     ]
    }
   ],
   "source": [
    "# --- OFFERS EVENTS ---\n",
    "offers_events_dir = f\"{DATASET_PATH_SMALL}/offers/events\"\n",
    "offers_events_files = dataset_files.get(offers_events_dir, [])\n",
    "\n",
    "df_offers_events = load_dataframe_from_partitions_safe(offers_events_files, limit=5)\n",
    "\n",
    "if df_offers_events is not None:\n",
    "    print(\"Validation Check:\")\n",
    "    print(df_offers_events.isnull().sum())\n",
    "    print(f\"\\nTotal Missing Values: {df_offers_events.isnull().sum().sum()}\")\n",
    "    print(f\"Shape: {df_offers_events.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have addressed the critical data quality issues identified in the EDA:\n",
    "1.  Recovered access to tables with broken schemas (Brands, Marketplace, Reviews).\n",
    "2.  Standardized handling of missing values across Users and Items.\n",
    "3.  Ensured data integrity by removing duplicates and invalid entries.\n",
    "\n",
    "The datasets are now ready for downstream feature engineering and modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
