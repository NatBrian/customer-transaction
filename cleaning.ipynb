{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning: T-ECD E-Commerce Transaction Dataset\n",
    "\n",
    "## Project Context\n",
    "\n",
    "This notebook documents the data cleaning process I performed on the T-ECD (Transactional E-Commerce Dataset) following exploratory data analysis.\n",
    "\n",
    "## Methodological Note\n",
    "\n",
    "- **Confirmed data errors**: Issues with direct evidence (e.g., missing value counts, duplicate counts, error messages)\n",
    "- **Assumptions**: Choices I made in the absence of complete information (e.g., imputation strategies)\n",
    "- **Open questions**: Uncertainties that remain unresolved (e.g., root causes of missing data)\n",
    "\n",
    "## Dataset Overview (as of 2025-12-21)\n",
    "\n",
    "The analysis revealed the following structure:\n",
    "\n",
    "| Table | Rows | Key Issues Identified |\n",
    "|-------|------|----------------------|\n",
    "| Users | 3.5M | Missing demographics (1.7% region, 0.1% cluster) |\n",
    "| Brands | 24.5K | Schema error in embeddings, 46 duplicates |\n",
    "| Retail Items | 250K | Missing categories (3.8%), prices (10.6%) |\n",
    "| Retail Events | 1.9M | No issues detected |\n",
    "| Marketplace Items | 2.3M | Schema error, missing values unknown (file load failed in analysis) |\n",
    "| Marketplace Events | 2.6M | Missing subdomain (0.04%) |\n",
    "| Offers Items | 22K | Missing brand_id (2.4%) |\n",
    "| Offers Events | 14.8M | No issues detected |\n",
    "| Reviews | 11.5K | Schema error in embeddings |\n",
    "| Payments Events | 11.5M | Missing brand_id (57.3%), price (0.004%) |\n",
    "| Payments Receipts | 1.1M | Missing brand_id (90%), price (1.3%) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Fix console encoding for Windows systems\n",
    "if sys.platform == 'win32':\n",
    "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File operations\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data loading\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset repository\n",
    "REPO_ID = \"t-tech/T-ECD\"\n",
    "REPO_TYPE = \"dataset\"\n",
    "\n",
    "# Local paths\n",
    "CACHE_DIR = \"dataset_cache\"\n",
    "OUTPUT_DIR = \"cleaned_data\"\n",
    "DATASET_PATH_SMALL = \"dataset/small\"\n",
    "DATASET_PATH_FULL = \"dataset/full\"\n",
    "\n",
    "# Partition limits to manage memory\n",
    "# I set these conservatively to ensure the notebook runs on typical hardware\n",
    "DATASET_SMALL_NUM_PARTITIONS_TO_LOAD = 5\n",
    "DATASET_FULL_NUM_PARTITIONS_TO_LOAD = 1\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Helper functions to standardize the cleaning workflow. These functions handle:\n",
    "1. **Loading**: Parquet loading with fallback logic for schema errors\n",
    "2. **Evidence display**: Showing concrete examples of data quality issues before cleaning\n",
    "3. **Validation**: Quantifying the impact of cleaning operations\n",
    "4. **Persistence**: Saving cleaned datasets for downstream use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_remote_parquet_safe(filename, columns_to_exclude=None):\n",
    "    \"\"\"\n",
    "    Loads a parquet file from Hugging Face, handling schema errors gracefully.\n",
    "    \n",
    "    Args:\n",
    "        filename: Path to parquet file in the repo\n",
    "        columns_to_exclude: List of columns to skip (e.g., corrupted embeddings)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame or None if loading fails\n",
    "    \"\"\"\n",
    "    print(f\"Loading {filename}...\")\n",
    "    try:\n",
    "        local_path = hf_hub_download(\n",
    "            repo_id=REPO_ID,\n",
    "            filename=filename,\n",
    "            repo_type=REPO_TYPE,\n",
    "            local_dir=CACHE_DIR,\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        \n",
    "        if columns_to_exclude:\n",
    "            import pyarrow.parquet as pq\n",
    "            schema = pq.read_schema(local_path)\n",
    "            all_cols = [name for name in schema.names if not name.startswith('__')]\n",
    "            use_cols = [c for c in all_cols if c not in columns_to_exclude]\n",
    "            print(f\"  Excluding columns: {columns_to_exclude}\")\n",
    "            print(f\"  Reading columns: {use_cols}\")\n",
    "            df = pd.read_parquet(local_path, columns=use_cols)\n",
    "            return df\n",
    "        \n",
    "        # Try loading all columns\n",
    "        try:\n",
    "            df = pd.read_parquet(local_path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"  Standard load failed: {e}\")\n",
    "            import pyarrow.parquet as pq\n",
    "            schema = pq.read_schema(local_path)\n",
    "            all_cols = [name for name in schema.names if not name.startswith('__')]\n",
    "            if 'embedding' in all_cols:\n",
    "                print(f\"  Retrying without 'embedding' column...\")\n",
    "                use_cols = [c for c in all_cols if c != 'embedding']\n",
    "                df = pd.read_parquet(local_path, columns=use_cols)\n",
    "                print(\"  Success (with exclusions).\")\n",
    "                return df\n",
    "            else:\n",
    "                raise e\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading/loading {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_dataframe_from_partitions_safe(file_list, limit=None, columns_to_exclude=None):\n",
    "    \"\"\"\n",
    "    Loads and concatenates multiple parquet partition files.\n",
    "    \n",
    "    Args:\n",
    "        file_list: List of partition file paths\n",
    "        limit: Maximum number of partitions to load (None = all)\n",
    "        columns_to_exclude: Columns to skip in all partitions\n",
    "    \n",
    "    Returns:\n",
    "        Concatenated DataFrame or None\n",
    "    \"\"\"\n",
    "    if not file_list:\n",
    "        print(\"No files to load.\")\n",
    "        return None\n",
    "    \n",
    "    files_to_load = file_list[:limit] if limit else file_list\n",
    "    print(f\"Loading {len(files_to_load)} partitions (out of {len(file_list)} available)...\")\n",
    "    \n",
    "    dfs = []\n",
    "    for f in files_to_load:\n",
    "        df = load_remote_parquet_safe(f, columns_to_exclude=columns_to_exclude)\n",
    "        if df is not None:\n",
    "            dfs.append(df)\n",
    "    \n",
    "    if not dfs:\n",
    "        print(\"No dataframes loaded successfully.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Concatenating partitions...\")\n",
    "    full_df = pd.concat(dfs, ignore_index=True)\n",
    "    return full_df\n",
    "\n",
    "\n",
    "def show_evidence(df, name, columns_to_check=None):\n",
    "    \"\"\"\n",
    "    Display evidence of data quality issues before cleaning.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to examine\n",
    "        name: Dataset name for reporting\n",
    "        columns_to_check: Specific columns to highlight (None = all)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVIDENCE: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    \n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\n Missing Values Found:\")\n",
    "        missing_df = pd.DataFrame({\n",
    "            'Column': missing.index,\n",
    "            'Missing Count': missing.values,\n",
    "            'Missing %': missing_pct.values\n",
    "        })\n",
    "        missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "        print(missing_df.to_string(index=False))\n",
    "        \n",
    "        # Show sample rows with missing values for key columns\n",
    "        if columns_to_check:\n",
    "            for col in columns_to_check:\n",
    "                if col in df.columns and df[col].isnull().sum() > 0:\n",
    "                    print(f\"\\n  Sample rows with missing '{col}':\")\n",
    "                    print(df[df[col].isnull()].head(2).to_string())\n",
    "    else:\n",
    "        print(f\"\\nNo missing values detected\")\n",
    "    \n",
    "    # Duplicates (skip unhashable columns)\n",
    "    try:\n",
    "        hashable_cols = [c for c in df.columns \n",
    "                        if df[c].dtype != 'object' or \n",
    "                        not df[c].apply(lambda x: isinstance(x, (list, np.ndarray))).any()]\n",
    "        dupes = df[hashable_cols].duplicated().sum()\n",
    "        if dupes > 0:\n",
    "            print(f\"\\nDuplicates Found: {dupes} rows\")\n",
    "        else:\n",
    "            print(f\"\\nNo duplicates detected\")\n",
    "    except:\n",
    "        print(f\"\\nDuplicate check skipped (unhashable columns)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def validate_cleaning(df_before, df_after, name):\n",
    "    \"\"\"\n",
    "    Validate the impact of cleaning operations.\n",
    "    \n",
    "    Args:\n",
    "        df_before: DataFrame before cleaning\n",
    "        df_after: DataFrame after cleaning\n",
    "        name: Dataset name for reporting\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"VALIDATION: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    rows_dropped = len(df_before) - len(df_after)\n",
    "    print(f\"\\nRows: {len(df_before):,} → {len(df_after):,} (Dropped: {rows_dropped:,})\")\n",
    "    \n",
    "    missing_before = df_before.isnull().sum().sum()\n",
    "    missing_after = df_after.isnull().sum().sum()\n",
    "    print(f\"Missing Values: {missing_before:,} → {missing_after:,}\")\n",
    "    \n",
    "    # Duplicates\n",
    "    try:\n",
    "        hashable_cols_before = [c for c in df_before.columns \n",
    "                               if df_before[c].dtype != 'object' or \n",
    "                               not df_before[c].apply(lambda x: isinstance(x, (list, np.ndarray))).any()]\n",
    "        hashable_cols_after = [c for c in df_after.columns \n",
    "                              if df_after[c].dtype != 'object' or \n",
    "                              not df_after[c].apply(lambda x: isinstance(x, (list, np.ndarray))).any()]\n",
    "        \n",
    "        dupes_before = df_before[hashable_cols_before].duplicated().sum()\n",
    "        dupes_after = df_after[hashable_cols_after].duplicated().sum()\n",
    "        print(f\"Duplicates: {dupes_before:,} → {dupes_after:,}\")\n",
    "    except:\n",
    "        print(f\"Duplicates: (skipped - unhashable columns)\")\n",
    "    \n",
    "    if missing_after == 0 and rows_dropped >= 0:\n",
    "        print(f\"\\nCLEANING SUCCESSFUL\")\n",
    "    else:\n",
    "        print(f\"\\nWARNING: Review cleaning results\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def save_cleaned_data(df, name):\n",
    "    \"\"\"\n",
    "    Save cleaned dataset to parquet file.\n",
    "    \n",
    "    Args:\n",
    "        df: Cleaned DataFrame\n",
    "        name: Dataset name for filename\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"{name.lower().replace(' ', '_')}_clean.parquet\")\n",
    "    df.to_parquet(output_file, index=False)\n",
    "    print(f\"Saved: {output_file} ({len(df):,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indexing remote dataset files...\")\n",
    "all_files = list_repo_files(repo_id=REPO_ID, repo_type=REPO_TYPE)\n",
    "dataset_files = defaultdict(list)\n",
    "\n",
    "for f in all_files:\n",
    "    if f.endswith(\".pq\"):\n",
    "        dirname = os.path.dirname(f).replace(\"\\\\\", \"/\")\n",
    "        dataset_files[dirname].append(f)\n",
    "\n",
    "print(f\"Indexed {len(dataset_files)} directories containing {sum(len(files) for files in dataset_files.values())} parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Users Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "In `analysis.ipynb`:\n",
    "- `socdem_cluster`: 5,153 missing values (0.1%)\n",
    "- `region`: 58,917 missing values (1.7%)\n",
    "- No duplicate records\n",
    "\n",
    "The missing values are legitimate gaps in the demographic data—some users have unknown sociodemographic clusters or regions.\n",
    "\n",
    "### Decision: Imputation Strategy\n",
    "\n",
    "- To handle missing demographic data, dropping 58,917 users (1.7% of the base) was rejected because it would break foreign key relationships and discard behavioral data. \n",
    "- Imputing with the statistical mode was rejected due to potential bias, as missing data may be non-random and represent systematically different users. \n",
    "- Instead, we impute missing values with a sentinel value of -1, which preserves all records, maintains referential integrity, and allows downstream analyses to handle or exclude unknown demographics. The sentinel value is safe, as all original demographic codes are non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load users table\n",
    "print(\"Loading users table...\")\n",
    "users_path = f\"{DATASET_PATH_SMALL}/users.pq\"\n",
    "df_users = load_remote_parquet_safe(users_path)\n",
    "\n",
    "if df_users is not None:\n",
    "    # Display evidence of data quality issues\n",
    "    show_evidence(df_users, \"Users (Before Cleaning)\", \n",
    "                 columns_to_check=['socdem_cluster', 'region'])\n",
    "    \n",
    "    # Apply cleaning: impute missing demographics with -1\n",
    "    print(\"Applying imputation strategy: missing demographics -> -1\")\n",
    "    df_users_clean = df_users.copy()\n",
    "    df_users_clean['socdem_cluster'] = df_users_clean['socdem_cluster'].fillna(-1)\n",
    "    df_users_clean['region'] = df_users_clean['region'].fillna(-1)\n",
    "    \n",
    "    # Validate cleaning impact\n",
    "    validate_cleaning(df_users, df_users_clean, \"Users\")\n",
    "    \n",
    "    # Verify complete resolution\n",
    "    assert df_users_clean.isnull().sum().sum() == 0, \"Cleaning failed: missing values remain\"\n",
    "    \n",
    "    # Save cleaned table\n",
    "    save_cleaned_data(df_users_clean, \"users\")\n",
    "    print(f\"Saved {len(df_users_clean):,} cleaned user records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Brands Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "In `analysis.ipynb`:\n",
    "1. **Schema error**: `embedding` column failed to load with error \"Expected all lists to be of size=300 but index 1 had size=0\"\n",
    "2. **46 duplicate brand_id entries**: Violates the expected primary key constraint\n",
    "\n",
    "The embedding column contains 300-dimensional vector representations of brands, intended for similarity calculations in recommendation systems. However, the source data contains inconsistent list lengths (empty lists mixed with 300-element lists), causing standard PyArrow loaders to fail schema validation before data can be accessed.\n",
    "\n",
    "### Decision: Embedding Column Strategy\n",
    "\n",
    "- Attempting low-level PyArrow recovery was rejected due to operational complexity and uncertain benefit, while replacing missing embeddings with dummy zero vectors was rejected because it would mislead models into treating them as legitimate brand representations. \n",
    "- Instead, the embedding column is excluded, preserving brand_id metadata and explicitly acknowledging the absence of embeddings. Downstream analyses must obtain embeddings from an alternative source or proceed without brand similarity features.\n",
    "\n",
    "### Decision: Duplicate Resolution\n",
    "\n",
    "- Among 46 duplicate brand_id values, manual investigation was rejected as unscalable without domain knowledge. \n",
    "- Keeping the first occurrence was chosen to ensure deterministic, reproducible behavior, prioritizing uniqueness over which specific duplicate is retained. Differences between duplicate records are assumed negligible, given minor metadata variations and the lack of additional identifying information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load brands table (exclude corrupted embedding column)\n",
    "print(\"Loading brands table...\")\n",
    "brands_path = f\"{DATASET_PATH_SMALL}/brands.pq\"\n",
    "df_brands = load_remote_parquet_safe(brands_path, columns_to_exclude=['embedding'])\n",
    "\n",
    "if df_brands is not None:\n",
    "    # Display evidence\n",
    "    show_evidence(df_brands, \"Brands (Before Cleaning)\")\n",
    "    \n",
    "    # Show duplicate evidence explicitly\n",
    "    duplicates_count = df_brands.duplicated(subset=['brand_id']).sum()\n",
    "    print(f\"\\nDuplicate brand_id records: {duplicates_count}\")\n",
    "    if duplicates_count > 0:\n",
    "        print(\"Sample duplicates:\")\n",
    "        duplicate_ids = df_brands[df_brands.duplicated(subset=['brand_id'], keep=False)]['brand_id'].unique()[:3]\n",
    "        print(df_brands[df_brands['brand_id'].isin(duplicate_ids)].sort_values('brand_id'))\n",
    "    \n",
    "    # Remove duplicates\n",
    "    print(\"\\nApplying deduplication: keeping first occurrence of each brand_id\")\n",
    "    df_brands_clean = df_brands.drop_duplicates(subset=['brand_id'], keep='first').copy()\n",
    "    \n",
    "    # Validate\n",
    "    validate_cleaning(df_brands, df_brands_clean, \"Brands\")\n",
    "    save_cleaned_data(df_brands_clean, \"brands\")\n",
    "    print(f\"Saved {len(df_brands_clean):,} unique brand records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Retail Items Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "From `analysis.ipynb`:\n",
    "- `category`: 9,585 missing (3.8%)\n",
    "- `subcategory`: 9,585 missing (3.8%)\n",
    "- `price`: 26,489 missing (10.6%)\n",
    "\n",
    "Category and subcategory missingness is perfectly correlated—when one is missing, both are missing. This suggests these items were not fully cataloged during data collection.\n",
    "\n",
    "### Decision: Category/Subcategory Strategy\n",
    "\n",
    "- Dropping 3.8% of the catalog was rejected to avoid unnecessarily reducing analytic scope.\n",
    "- Missing categorical attributes are imputed with the explicit label \"Unknown\" rather than dropping records or using mode values. This allows product catalog analyses to group \"Unknown\" items separately while retaining price and brand information, enabling revenue analysis even without category data. \n",
    "\n",
    "### Decision: Price Strategy\n",
    "\n",
    "- Rows with missing prices are dropped rather than imputed. \n",
    "- Imputation would introduce severe bias due to diverse product categories and missing category labels—mean or median values would misrepresent items like low-cost food or high-cost home improvement products. Price is critical for revenue, affordability, and pricing analyses, so retaining imputed prices would compromise validity. \n",
    "- The resulting 10.6% data loss is an acceptable quality-coverage trade-off, prioritizing analytic integrity for research and model training, though a production system might make a different choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load retail items\n",
    "print(\"Loading retail items...\")\n",
    "retail_items_path = f\"{DATASET_PATH_SMALL}/retail/items.pq\"\n",
    "df_retail_items = load_remote_parquet_safe(retail_items_path)\n",
    "\n",
    "if df_retail_items is not None:\n",
    "    show_evidence(df_retail_items, \"Retail Items (Before Cleaning)\",\n",
    "                 columns_to_check=['category', 'subcategory', 'price'])\n",
    "    \n",
    "    # Apply cleaning\n",
    "    print(\"Applying cleaning strategy:\")\n",
    "    print(\"  - category/subcategory: impute 'Unknown'\")\n",
    "    print(\"  - price: drop rows with missing values\")\n",
    "    \n",
    "    df_retail_clean = df_retail_items.copy()\n",
    "    df_retail_clean['category'] = df_retail_clean['category'].fillna(\"Unknown\")\n",
    "    df_retail_clean['subcategory'] = df_retail_clean['subcategory'].fillna(\"Unknown\")\n",
    "    df_retail_clean = df_retail_clean.dropna(subset=['price'])\n",
    "    \n",
    "    validate_cleaning(df_retail_items, df_retail_clean, \"Retail Items\")\n",
    "    save_cleaned_data(df_retail_clean, \"retail_items\")\n",
    "    print(f\"Retained {len(df_retail_clean):,} items ({len(df_retail_clean)/len(df_retail_items)*100:.1f}% of original catalog)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Retail Events Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "From `analysis.ipynb`: No data quality issues detected. All columns complete, no duplicates.\n",
    "\n",
    "### Decision: No Cleaning Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate retail events\n",
    "print(\"Loading retail events (validation only)...\")\n",
    "retail_events_dir = f\"{DATASET_PATH_SMALL}/retail/events\"\n",
    "retail_events_files = dataset_files.get(retail_events_dir, [])\n",
    "df_retail_events = load_dataframe_from_partitions_safe(\n",
    "    retail_events_files, \n",
    "    limit=DATASET_SMALL_NUM_PARTITIONS_TO_LOAD\n",
    ")\n",
    "\n",
    "if df_retail_events is not None:\n",
    "    show_evidence(df_retail_events, \"Retail Events (Validation)\")\n",
    "    assert df_retail_events.isnull().sum().sum() == 0, \"Unexpected data quality issues\"\n",
    "    print(\"Validation passed: no cleaning required\")\n",
    "    save_cleaned_data(df_retail_events, \"retail_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Marketplace Items Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "From `analysis.ipynb`:\n",
    "- `subcategory`: 1,233,023 missing (53%)\n",
    "- `category`: 966,395 missing (42%)  \n",
    "- `price`: 2,882 missing (0.12%)\n",
    "\n",
    "### Decision: Imputation Strategy\n",
    "\n",
    "- For marketplace items, missing categorical attributes are imputed as \"Unknown,\" consistent with the Retail Items strategy. Missing prices, which affect only 0.12% of records, are imputed with -1 rather than dropped. \n",
    "- The lower missingness shifts the coverage-quality trade-off: dropping such a small fraction has minimal impact, but imputation maximizes data retention for the larger marketplace catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load marketplace items\n",
    "print(\"Loading marketplace items (embedding excluded due to schema error)...\")\n",
    "mp_items_path = f\"{DATASET_PATH_SMALL}/marketplace/items.pq\"\n",
    "df_mp_items = load_remote_parquet_safe(mp_items_path, columns_to_exclude=['embedding'])\n",
    "\n",
    "if df_mp_items is not None:\n",
    "    show_evidence(df_mp_items, \"Marketplace Items\")\n",
    "    \n",
    "    # Apply cleaning if needed\n",
    "    missing_count = df_mp_items.isnull().sum().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Discovered {missing_count} missing values (not in original analysis)\")\n",
    "        print(\"Applying imputation strategy\")\n",
    "        \n",
    "        df_mp_items_clean = df_mp_items.copy()\n",
    "        for col in df_mp_items.columns:\n",
    "            if df_mp_items[col].isnull().sum() > 0:\n",
    "                if df_mp_items[col].dtype == 'object':\n",
    "                    df_mp_items_clean[col] = df_mp_items_clean[col].fillna(\"Unknown\")\n",
    "                else:\n",
    "                    df_mp_items_clean[col] = df_mp_items_clean[col].fillna(-1)\n",
    "        \n",
    "        validate_cleaning(df_mp_items, df_mp_items_clean, \"Marketplace Items\")\n",
    "        save_cleaned_data(df_mp_items_clean, \"marketplace_items\")\n",
    "    else:\n",
    "        print(\"No cleaning required\")\n",
    "        save_cleaned_data(df_mp_items, \"marketplace_items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Marketplace Events Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "From `analysis.ipynb`:\n",
    "- `subdomain`: 1,115 missing (0.04%)\n",
    "- All other columns complete\n",
    "\n",
    "### Decision: Imputation Strategy\n",
    "\n",
    "I impute missing subdomain values with \"Unknown\" to preserve complete event logs for user journey analysis. Events represent user actions and are valuable regardless of the subdomain context. Dropping 1,115 event records would create gaps in behavioral sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load marketplace events\n",
    "print(\"Loading marketplace events...\")\n",
    "mp_events_dir = f\"{DATASET_PATH_SMALL}/marketplace/events\"\n",
    "mp_events_files = dataset_files.get(mp_events_dir, [])\n",
    "df_mp_events = load_dataframe_from_partitions_safe(\n",
    "    mp_events_files, \n",
    "    limit=DATASET_SMALL_NUM_PARTITIONS_TO_LOAD\n",
    ")\n",
    "\n",
    "if df_mp_events is not None:\n",
    "    show_evidence(df_mp_events, \"Marketplace Events (Before Cleaning)\", \n",
    "                 columns_to_check=['subdomain'])\n",
    "    \n",
    "    # Apply cleaning\n",
    "    print(\"Imputing missing subdomain with 'Unknown'\")\n",
    "    df_mp_events_clean = df_mp_events.copy()\n",
    "    df_mp_events_clean['subdomain'] = df_mp_events_clean['subdomain'].fillna(\"Unknown\")\n",
    "    \n",
    "    # Validate\n",
    "    validate_cleaning(df_mp_events, df_mp_events_clean, \"Marketplace Events\")\n",
    "    save_cleaned_data(df_mp_events_clean, \"marketplace_events\")\n",
    "    print(f\"Saved {len(df_mp_events_clean):,} event records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Offers Items Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "From `analysis.ipynb`:\n",
    "- `brand_id`: 542 missing (2.4%)\n",
    "- All other columns complete\n",
    "\n",
    "### Decision: Imputation Strategy\n",
    "\n",
    "I impute missing brand_id values with -1 rather than dropping records. Some offers may represent unbranded promotions or platform-wide deals that do not map to specific brands. The 2.4% missingness is modest, and offer-level attributes (item_id, offer terms) remain valid for promotion effectiveness analysis independent of brand attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load offers items\n",
    "print(\"Loading offers items...\")\n",
    "offers_items_path = f\"{DATASET_PATH_SMALL}/offers/items.pq\"\n",
    "df_offers_items = load_remote_parquet_safe(offers_items_path)\n",
    "\n",
    "if df_offers_items is not None:\n",
    "    show_evidence(df_offers_items, \"Offers Items (Before Cleaning)\", \n",
    "                 columns_to_check=['brand_id'])\n",
    "    \n",
    "    # Apply cleaning\n",
    "    print(\"Imputing missing brand_id with -1\")\n",
    "    df_offers_items_clean = df_offers_items.copy()\n",
    "    df_offers_items_clean['brand_id'] = df_offers_items_clean['brand_id'].fillna(-1)\n",
    "    \n",
    "    # Validate\n",
    "    validate_cleaning(df_offers_items, df_offers_items_clean, \"Offers Items\")\n",
    "    save_cleaned_data(df_offers_items_clean, \"offers_items\")\n",
    "    print(f\"Saved {len(df_offers_items_clean):,} offer records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Offers Events Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "From `analysis.ipynb`:\n",
    "- No missing values detected\n",
    "- No duplicate records\n",
    "\n",
    "### Decision: No Cleaning Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate offers events\n",
    "print(\"Loading offers events (validation only)...\")\n",
    "offers_events_dir = f\"{DATASET_PATH_SMALL}/offers/events\"\n",
    "offers_events_files = dataset_files.get(offers_events_dir, [])\n",
    "df_offers_events = load_dataframe_from_partitions_safe(\n",
    "    offers_events_files, \n",
    "    limit=DATASET_SMALL_NUM_PARTITIONS_TO_LOAD\n",
    ")\n",
    "\n",
    "if df_offers_events is not None:\n",
    "    show_evidence(df_offers_events, \"Offers Events (Validation)\")\n",
    "    \n",
    "    # Assertion: verify no issues present\n",
    "    assert df_offers_events.isnull().sum().sum() == 0, \"Unexpected missing values detected\"\n",
    "    \n",
    "    print(\"Validation passed: no cleaning required\")\n",
    "    save_cleaned_data(df_offers_events, \"offers_events\")\n",
    "    print(f\"Saved {len(df_offers_events):,} event records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reviews Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "From `analysis.ipynb`:\n",
    "- Embedding column has schema error (excluded during load)\n",
    "- All data columns (timestamp, user_id, brand_id, rating) complete\n",
    "- No duplicate records\n",
    "\n",
    "### Decision: No Cleaning Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate reviews (exclude corrupted embeddings)\n",
    "print(\"Loading reviews (embedding column excluded due to schema error)...\")\n",
    "reviews_dir = f\"{DATASET_PATH_SMALL}/reviews\"\n",
    "review_files = dataset_files.get(reviews_dir, [])\n",
    "df_reviews = load_dataframe_from_partitions_safe(\n",
    "    review_files, \n",
    "    limit=DATASET_SMALL_NUM_PARTITIONS_TO_LOAD, \n",
    "    columns_to_exclude=['embedding']\n",
    ")\n",
    "\n",
    "if df_reviews is not None:\n",
    "    show_evidence(df_reviews, \"Reviews (Validation)\")\n",
    "    \n",
    "    # Verify completeness\n",
    "    missing_count = df_reviews.isnull().sum().sum()\n",
    "    if missing_count == 0:\n",
    "        print(\"Validation passed: no missing values in data columns\")\n",
    "        save_cleaned_data(df_reviews, \"reviews\")\n",
    "        print(f\"Saved {len(df_reviews):,} review records\")\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected {missing_count} missing values detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Payments Events Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "From `analysis.ipynb`:\n",
    "- `brand_id`: 6,583,207 missing (57.3%)\n",
    "- `price`: 462 missing (0.004%)\n",
    "\n",
    "### Decision: Differential Strategy\n",
    "\n",
    "**For brand_id (57% missing):** I impute with -1 rather than dropping records. The extraordinarily high missingness likely reflects business logic—transactions may involve bundled items, generic products, or services without individual brand attribution. Dropping 57% of payment data would destroy the revenue analysis capability.\n",
    "\n",
    "**For price (0.004% missing):** I drop rows with missing prices. Price is critical for payment analysis, and the 462 affected records represent negligible coverage loss (0.004%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load payments events\n",
    "print(\"Loading payments events...\")\n",
    "pay_events_dir = f\"{DATASET_PATH_FULL}/payments/events\"\n",
    "pay_events_files = dataset_files.get(pay_events_dir, [])\n",
    "df_pay_events = load_dataframe_from_partitions_safe(\n",
    "    pay_events_files, \n",
    "    limit=DATASET_FULL_NUM_PARTITIONS_TO_LOAD\n",
    ")\n",
    "\n",
    "if df_pay_events is not None:\n",
    "    show_evidence(df_pay_events, \"Payments Events (Before Cleaning)\", \n",
    "                 columns_to_check=['brand_id', 'price'])\n",
    "    \n",
    "    # Apply cleaning\n",
    "    print(\"Applying differential strategy:\")\n",
    "    print(\"  - brand_id (57% missing): impute with -1\")\n",
    "    print(\"  - price (0.004% missing): drop rows\")\n",
    "    \n",
    "    df_pay_events_clean = df_pay_events.copy()\n",
    "    df_pay_events_clean['brand_id'] = df_pay_events_clean['brand_id'].fillna(-1)\n",
    "    df_pay_events_clean = df_pay_events_clean.dropna(subset=['price'])\n",
    "    \n",
    "    # Validate\n",
    "    validate_cleaning(df_pay_events, df_pay_events_clean, \"Payments Events\")\n",
    "    save_cleaned_data(df_pay_events_clean, \"payments_events\")\n",
    "    print(f\"Retained {len(df_pay_events_clean):,} payment events ({len(df_pay_events_clean)/len(df_pay_events)*100:.2f}% of original)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Payments Receipts Table\n",
    "\n",
    "### Evidence from Analysis\n",
    "\n",
    "From `analysis.ipynb`:\n",
    "- `brand_id`: 999,928 missing (90%)\n",
    "- `price`: 14,892 missing (1.3%)\n",
    "\n",
    "### Decision: Differential Strategy\n",
    "\n",
    "**For brand_id (90% missing):** I impute with -1. The extreme missingness parallels Payments Events and suggests that receipt-level brand attribution is often unavailable in the transaction system. Receipts capture item-level details via `approximate_item_id`, making the transaction data useful even without brand mapping. Dropping 90% of receipt data would be unacceptable.\n",
    "\n",
    "**For price (1.3% missing):** I drop rows with missing prices. Price is essential for receipt value calculation and revenue analysis. The 1.3% coverage loss is acceptable given the analytic importance of price integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load payments receipts\n",
    "print(\"Loading payments receipts...\")\n",
    "receipts_dir = f\"{DATASET_PATH_FULL}/payments/receipts\"\n",
    "receipt_files = dataset_files.get(receipts_dir, [])\n",
    "df_receipts = load_dataframe_from_partitions_safe(\n",
    "    receipt_files, \n",
    "    limit=DATASET_FULL_NUM_PARTITIONS_TO_LOAD\n",
    ")\n",
    "\n",
    "if df_receipts is not None:\n",
    "    show_evidence(df_receipts, \"Payments Receipts (Before Cleaning)\", \n",
    "                 columns_to_check=['brand_id', 'price'])\n",
    "    \n",
    "    # Apply cleaning\n",
    "    print(\"Applying differential strategy:\")\n",
    "    print(\"  - brand_id (90% missing): impute with -1\")\n",
    "    print(\"  - price (1.3% missing): drop rows\")\n",
    "    \n",
    "    df_receipts_clean = df_receipts.copy()\n",
    "    df_receipts_clean['brand_id'] = df_receipts_clean['brand_id'].fillna(-1)\n",
    "    df_receipts_clean = df_receipts_clean.dropna(subset=['price'])\n",
    "    \n",
    "    # Validate\n",
    "    validate_cleaning(df_receipts, df_receipts_clean, \"Payments Receipts\")\n",
    "    save_cleaned_data(df_receipts_clean, \"payments_receipts\")\n",
    "    print(f\"Retained {len(df_receipts_clean):,} receipt records ({len(df_receipts_clean)/len(df_receipts)*100:.1f}% of original)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection and Limitations\n",
    "\n",
    "### What I Did Not Clean\n",
    "\n",
    "Several common data cleaning steps were **intentionally omitted** because they lacked evidence:\n",
    "\n",
    "1. **Outlier detection and removal**: The analysis showed no evidence of extreme or implausible values. Price distributions, while varied, appeared consistent with a diverse product catalog. Removing outliers without specific evidence would risk discarding legitimate premium or bulk-purchase items.\n",
    "\n",
    "2. **Data type conversions**: All columns had appropriate types (`uint64` for IDs, `float64` for prices, `object` for text, `timedelta64` for timestamps). No conversions were necessary.\n",
    "\n",
    "### Confirmed vs. Assumed\n",
    "\n",
    "**Confirmed data errors** (with explicit evidence):\n",
    "- Missing values (counts verified)\n",
    "- Duplicate brand records (count verified)\n",
    "- Embedding schema corruption (error message verified)\n",
    "\n",
    "**Assumptions** (necessary but unverified):\n",
    "- -1 does not conflict with legitimate ID values\n",
    "- First-occurrence duplicate resolution is acceptable\n",
    "\n",
    "### Cleaned Dataset Availability\n",
    "\n",
    "All cleaned tables are saved in `cleaned_data/` as Parquet files, ready for modeling and analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
